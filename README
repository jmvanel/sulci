=====
Sulci
=====

Sulci is a French text mining tool, initially designed for the analysis of
the corpus and thesaurus of `Libération <http://www.liberation.fr/>`_, a 
french newspaper.

This code is "work in progress"; it's not used (nor usable) in production yet.

It provides 4 algorithms, designed to be run in sequence: each algorithm
needs the data provided by the previous one.

Each algorithm must be *trained*. What does that mean?

#. The algorithm will process your input, and give you some output.
#. You check the output, and if you find invalid results, you fix them.
#. You submit the corrected output to the algorithm. The algorithm will
   analyze its mistakes, and change its processing rules/weights/triggers/...
   in order to avoid making those mistakes again in the future.
#. Repeat from step 1 with another input, until you are satisfied with the
   result.

.. note:: 
   You can also skip step 1 & 2, and directly submit "correct output" to
   the algorithm; it will compare your (supposedly) valid output to 
   his calculated output, and adjust accordingly.

.. FIXME: le semanticaltagger, comment il utilise les infos grammaticales?

Before running the first algorithm, the text is split into tokens
(words, symbols, punctuation marks, etc.), using simple regular expressions.


Part-of-speech tagging
----------------------

The PosTagger finds out the "POS tag", or "lexical class", or "lexical 
category" of each word. 
The algorithm used is similar to the `Brill POS-tagging algorithm
<http://en.wikipedia.org/wiki/Brill_tagger>`_.

Some possible classes are:

* VCJ:sg (Verbe ConJugé singulier),
* PAR:pl (PARticipe pluriel),
* PREP (PREPosition),
* etc.

To see more available classes, see in base.py the methods named is_<something>.

The format used is a plain text file; tokens are separated by spaces;
each token is annotated with its POS tag, separated with a slash. Example::

  Mon/DTN:sg cher/ADJ:sg Fred/SBP:sg ,/, Je/PRV:sg quitte/VCJ:sg 
  Paris/SBP:sg demain/ADV matin/SBC:sg ./. 

This format combines "input" and "output": the input is the token, the output
is the POS tag.

Check "corpus/*.crp" to see more examples of "valid output".


Lemmatization
-------------

The Lemmatizer tries to find the *Lemma* of each word. The lemma of a word
is almost similar to its stem. A few examples:

* "mangerons" lemma is "manger" (infinitive form of the verb)
* "bonnes" lemma is "bon" (masculine singular form of the adjective)
* "meilleur" lemma is "bon" (superlative form; this one cannot be inferred
  by stemming only, and requires a dictionary lookup)

.. FIXME: ce dernier exemple est-il valide ?

The format used is similar to the one of the PosTagger, but each token
is annotated by both its POS tag and its lemma. Example::

  «/« Ce/PRV:sg/ce n'/ADV/ne est/ECJ:sg/être pas/ADV à/PREP moi/PRO:sg 
  de/PREP partir/VNCFF ./. Je/PRV:sg/je me/PRV:sg battrai/VCJ:sg/battre 
  jusqu'/PREP au/DTC:sg bout/SBC:sg ./. »/» 

If a word and its lemma are identical, the lemma is omitted. Note that
this is case-sensitive (as you can see on the first word of the above
excerpt).


Semantical tagging
------------------

The SemanticalTagger tries to find "collocations" -- i.e., sequence of tokens
that have a higher chance of appearing together. A bit like a keyword is 
a word with a high significance, a collocation is a sequence with high
significance. A few examples:

* Président de la République
* voiture électrique
* Barack Obama

The SemanticalTagger will actually use two different algorithms:

* an heuristics-based algorithm, scoring n-grams (sequences of words) with
  hand-crafted rules;
* a purely statistical algorithm, scoring n-grams according to their
  relatives frequences in the corpus.

The statistical algorithm uses `Pointwise mutual information 
<http://en.wikipedia.org/wiki/Pointwise_mutual_information>`_.

The efficiency of both algorithms is compared and FIXME: then what?


Semantical training
-------------------

..     * on fait ça pour 25 000 textes déjà tagués par un être humain, et on lie chaque entité clé (triggers) aux tags qui ont été posés (descriptors) => SemanticalTrainer


After the training phase...
---------------------------

Once all algorithms have been trained to a satisfactory level, they are 
ready to analyze new texts without your guidance (i.e., you won't have
to pre-tag those texts, indeed).

Steps 1 to 4 are run in sequence, and ...

    * pour extraire les descripteurs d'un nouveau texte, on refait les étapes 1-4, et là encore avec des stats on extraie les descripteurs les plus pertinents selon les déclencheurs rencontrés (entités clés) => SemanticalTagger


Installation
------------

You have to unzip the SQL fixtures (52Mo) and load them in DB.
After that, "python setup.py install" or put the sulci folder in your 
PYTHONPATH.

Add "sulci" to your INSTALLED_APPS.

Sample usage::

  >> from sulci.textmining import SemanticalTagger
  >> my_text = u"Voici mon magnifique texte en français qui parle de Charlie Chaplin en Europe."
  >> S = SemanticalTagger(my_text)
  >> S.descriptors()
  Charlie Chaplin
  Europe

Feel free to contribute any help!
